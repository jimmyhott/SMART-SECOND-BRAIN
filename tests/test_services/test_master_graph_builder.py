"""
Test script for MasterGraphBuilder workflow.

This module tests the Smart Second Brain master graph builder,
including node methods, graph compilation, and workflow execution.
"""

import pytest
import sys
import os
import tempfile
import shutil
from pathlib import Path
from unittest.mock import Mock, patch, mock_open, MagicMock
from typing import Dict, Any
from dotenv import load_dotenv

# Add the project root to the path for imports
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

# Import centralized logging
from shared.utils.logging_config import setup_test_logging

# Set up test logging
logger = setup_test_logging("test_master_graph_builder")

# Load environment variables from .env file
load_dotenv()

# Add the project root to the path for imports
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', '..'))

from agentic.core.knowledge_state import KnowledgeState
from agentic.workflows.master_graph_builder import MasterGraphBuilder

# Import real components for integration testing
try:
    from langchain_openai import ChatOpenAI, AzureChatOpenAI, OpenAIEmbeddings, AzureOpenAIEmbeddings
    from langchain_chroma import Chroma
    from langchain_text_splitters import RecursiveCharacterTextSplitter
    from langchain.schema import Document
    REAL_COMPONENTS_AVAILABLE = True
except ImportError:
    REAL_COMPONENTS_AVAILABLE = False
    logger.warning("âš ï¸  Real components not available. Install langchain packages for integration tests.")


class TestMasterGraphBuilder:
    """Test suite for MasterGraphBuilder class."""

    @pytest.fixture
    def mock_llm(self):
        """Create a mock LLM client."""
        mock = Mock()
        mock_response = Mock()
        mock_response.content = "This is a test answer generated by the LLM."
        mock.invoke.return_value = mock_response
        
        # Add support for chain operations (prompt | llm)
        def mock_or(other):
            chain_mock = Mock()
            chain_mock.invoke.return_value = mock_response
            return chain_mock
        mock.__or__ = mock_or
        
        # Also support direct chain invocation
        chain_mock = Mock()
        chain_mock.invoke.return_value = mock_response
        mock.chain = chain_mock
        
        return mock

    @pytest.fixture
    def mock_retriever(self):
        """Create a mock document retriever."""
        mock = Mock()
        # Create a serializable document object
        class MockDocument:
            def __init__(self):
                self.page_content = "This is a retrieved document for testing."
                self.metadata = {"source": "test", "page": 1}
        
        mock_doc = MockDocument()
        mock.get_relevant_documents.return_value = [mock_doc]
        return mock

    @pytest.fixture
    def mock_embedding_model(self):
        """Create a mock embedding model."""
        mock = Mock()
        # Return embeddings based on the number of input documents
        def mock_embed_documents(documents):
            return [[0.1, 0.2, 0.3] for _ in documents]
        mock.embed_documents.side_effect = mock_embed_documents
        return mock

    @pytest.fixture
    def mock_vectorstore(self):
        """Create a mock vector store."""
        mock = Mock()
        mock.add_texts = Mock()
        return mock

    @pytest.fixture
    def graph_builder(self, mock_llm, mock_embedding_model, mock_retriever, mock_vectorstore):
        """Create a MasterGraphBuilder instance with mocked dependencies."""
        return MasterGraphBuilder(
            llm=mock_llm,
            embedding_model=mock_embedding_model,
            retriever=mock_retriever,
            vectorstore=mock_vectorstore
        )

    @pytest.fixture
    def temp_chromadb_dir(self):
        """Create a temporary ChromaDB directory for testing."""
        temp_dir = tempfile.mkdtemp(prefix="test_chromadb_")
        logger.info(f"ðŸ§ª Created temporary ChromaDB directory: {temp_dir}")
        yield temp_dir
        # Cleanup after test
        if os.path.exists(temp_dir):
            shutil.rmtree(temp_dir)
            logger.info(f"ðŸ§¹ Cleaned up temporary ChromaDB directory: {temp_dir}")

    @pytest.fixture
    def real_graph_builder(self, temp_chromadb_dir):
        """Create a MasterGraphBuilder instance with real components and temporary ChromaDB."""
        if not REAL_COMPONENTS_AVAILABLE:
            pytest.skip("Real components not available")
        
        # Check for OpenAI API key
        openai_api_key = os.getenv("OPENAI_API_KEY")
        azure_endpoint = os.getenv("AZURE_OPENAI_ENDPOINT_URL")
        
        if not openai_api_key:
            pytest.skip("OPENAI_API_KEY not set in .env file")
        
        try:
            # Get model name from environment
            model_name = os.getenv("LLM_MODEL", "gpt-4o")
            
            # Use Azure OpenAI if endpoint is configured, otherwise use OpenAI
            if azure_endpoint and azure_endpoint != "https://your-resource-name.openai.azure.com/":
                logger.info(f"ðŸ”— Using Azure OpenAI endpoint: {azure_endpoint}")
                logger.info(f"ðŸ¤– Using model: {model_name}")
                
                # Get API version from environment
                api_version = os.getenv("API_VERSION", "2024-02-15-preview")
                
                # Use AzureChatOpenAI for Azure endpoints
                llm = AzureChatOpenAI(
                    azure_deployment=model_name,  # This is the deployment name
                    openai_api_version=api_version,
                    azure_endpoint=azure_endpoint,
                    openai_api_key=openai_api_key,
                    temperature=0
                )
            else:
                logger.info("ðŸ”— Using OpenAI API directly")
                logger.info(f"ðŸ¤– Using model: {model_name}")
                llm = ChatOpenAI(
                    model=model_name,
                    temperature=0,
                    openai_api_key=openai_api_key
                )
            
            # Initialize embedding model
            embedding_model = os.getenv("EMBEDDING_MODEL", "text-embedding-3-small")
            api_version = os.getenv("API_VERSION", "2024-02-15-preview")
            
            if azure_endpoint and azure_endpoint != "https://your-resource-name.openai.azure.com/":
                # Use AzureOpenAIEmbeddings for Azure endpoints
                deployment = "text-embedding-3-small"
                logger.info(f"ðŸ”— Using Azure OpenAI embeddings: {embedding_model}")
                logger.info(f"ðŸš€ Using deployment: {deployment}")
                embeddings = AzureOpenAIEmbeddings(
                    azure_deployment=deployment,
                    openai_api_version=api_version,
                    azure_endpoint=azure_endpoint,
                    openai_api_key=openai_api_key
                )
            else:
                embeddings = OpenAIEmbeddings(
                    model=embedding_model,
                    openai_api_key=openai_api_key
                )
            
            # Create MasterGraphBuilder with temporary ChromaDB directory
            builder = MasterGraphBuilder(
                llm=llm,
                embedding_model=embeddings,
                retriever=None,  # Will be set up in individual tests
                vectorstore=None,  # Will be set up in individual tests
                chromadb_dir=temp_chromadb_dir  # Use temporary directory
            )
            
            return builder
        except Exception as e:
            pytest.skip(f"Failed to initialize real components: {e}")

    @pytest.fixture
    def sample_documents(self):
        """Create sample documents for testing."""
        return [
            "The Smart Second Brain is an AI-powered knowledge management system that helps users organize and retrieve information efficiently.",
            "Knowledge graphs are powerful tools for representing relationships between concepts and entities in a structured format.",
            "LangGraph is a framework for building stateful, multi-actor applications with LLMs, enabling complex workflows and agent interactions.",
            "Vector databases store high-dimensional embeddings that enable semantic search and similarity matching across large document collections.",
            "Human-in-the-loop systems combine AI automation with human oversight to ensure quality and handle edge cases effectively."
        ]

    @pytest.fixture
    def sample_ingest_state(self):
        """Create a sample knowledge state for ingestion testing."""
        return KnowledgeState(
            query_type="ingest",
            raw_document="This is a test document.\n\nIt has multiple paragraphs.\n\nFor testing purposes.",
            metadata={"source": "test", "timestamp": "2024-01-01T00:00:00Z"}
        )

    @pytest.fixture
    def sample_query_state(self):
        """Create a sample knowledge state for query testing."""
        return KnowledgeState(
            query_type="query",
            user_input="What is the main topic of the document?",
            messages=[{"role": "user", "content": "What is the main topic of the document?"}]
        )

    def test_initialization(self, mock_llm, mock_retriever, mock_vectorstore):
        """Test MasterGraphBuilder initialization."""
        builder = MasterGraphBuilder(
            llm=mock_llm,
            retriever=mock_retriever,
            vectorstore=mock_vectorstore
        )
        
        assert builder.llm == mock_llm
        assert builder.retriever == mock_retriever
        assert builder.vectorstore == mock_vectorstore

    def test_input_router_ingest(self, graph_builder, sample_ingest_state):
        """Test input router for ingest query type."""
        result = graph_builder.input_router(sample_ingest_state)
        assert result == sample_ingest_state

    def test_input_router_query(self, graph_builder, sample_query_state):
        """Test input router for query query type."""
        result = graph_builder.input_router(sample_query_state)
        assert result == sample_query_state

    def test_input_router_unknown(self, graph_builder):
        """Test input router for unknown query type."""
        state = KnowledgeState(query_type="unknown")
        result = graph_builder.input_router(state)
        assert result == state
        assert state.status == "error"
        assert "Unknown query_type" in (state.logs or [])

    def test_chunk_doc_node(self, graph_builder, sample_ingest_state):
        """Test document chunking node."""
        result_state = graph_builder.chunk_doc_node(sample_ingest_state)
        
        assert result_state.chunks is not None
        assert len(result_state.chunks) >= 1  # At least one chunk
        # Check that the first chunk contains the document content
        assert "This is a test document." in result_state.chunks[0]

    def test_chunk_doc_node_empty_document(self, graph_builder):
        """Test document chunking with empty document."""
        state = KnowledgeState(query_type="ingest", raw_document="")
        result_state = graph_builder.chunk_doc_node(state)
        
        assert result_state.chunks is None

    def test_embed_node(self, graph_builder, sample_ingest_state):
        """Test embedding node with embedding model."""
        # First chunk the document
        state = graph_builder.chunk_doc_node(sample_ingest_state)
        result_state = graph_builder.embed_node(state)
        
        assert result_state.embeddings is not None
        assert len(result_state.embeddings) == len(result_state.chunks)
        # Check that embeddings are lists of floats
        assert all(isinstance(emb, list) for emb in result_state.embeddings)

    def test_embed_node_without_model(self, sample_ingest_state):
        """Test embedding node without embedding model (fallback behavior)."""
        builder = MasterGraphBuilder()  # No embedding model
        state = builder.chunk_doc_node(sample_ingest_state)
        result_state = builder.embed_node(state)
        
        assert result_state.embeddings is not None
        assert len(result_state.embeddings) == len(result_state.chunks)
        # Check that fallback embeddings are used
        assert all(emb == [0.1, 0.2] for emb in result_state.embeddings)

    @pytest.mark.integration
    def test_long_paragraph_chunk_embed_store(self, real_graph_builder):
        """Test processing a long paragraph through chunk, embed, and store workflow."""
        if not os.getenv("OPENAI_API_KEY"):
            pytest.skip("OPENAI_API_KEY not set in .env file")
        
        try:
            # Create a long paragraph for testing
            long_paragraph = """
            The Smart Second Brain is an advanced AI-powered knowledge management system that revolutionizes how individuals and organizations process, store, and retrieve information. Built on cutting-edge technologies including LangGraph for workflow orchestration, Azure OpenAI for natural language processing, and ChromaDB for vector storage, this platform provides intelligent document processing capabilities that go far beyond simple text storage. The system employs sophisticated text chunking algorithms that break down large documents into semantically meaningful pieces, ensuring that context is preserved while optimizing for embedding generation and retrieval. Each chunk is then processed through state-of-the-art embedding models like text-embedding-3-small, which converts text into high-dimensional vector representations that capture semantic meaning and enable powerful similarity searches. The vector database, powered by ChromaDB, stores these embeddings alongside rich metadata including source information, categorization tags, and temporal data, allowing for complex queries that can find relevant information across vast document collections. The platform's retrieval system uses advanced semantic search algorithms to find the most relevant chunks based on user queries, and its answer generation component leverages large language models to synthesize coherent, contextually appropriate responses. What sets this system apart is its human-in-the-loop architecture, which combines AI automation with human oversight to ensure quality, handle edge cases, and provide continuous learning capabilities. The system also features robust error handling, comprehensive logging, and scalable architecture that can handle everything from individual note-taking to enterprise-level knowledge management. Users can interact with the system through multiple interfaces, including natural language queries, document uploads, and API integrations, making it suitable for researchers, content creators, businesses, and anyone who needs to manage large amounts of information effectively. The platform's modular design allows for easy customization and extension, with components that can be swapped out or enhanced based on specific use cases and requirements.
            """
            
            # Create state with long paragraph
            state = KnowledgeState(
                query_type="ingest",
                raw_document=long_paragraph,
                source="test_long_paragraph",
                categories=["ai", "knowledge_management", "technology"],
                metadata={
                    "author": "test_user",
                    "timestamp": "2024-01-01T00:00:00Z",
                    "document_type": "technical_description"
                }
            )
            
            logger.info(f"ðŸ“„ Processing long paragraph ({len(long_paragraph)} characters)")
            
            # Build the graph with checkpointer
            graph = real_graph_builder.build()
            
            # Step 1: Run the complete ingestion workflow with checkpointer
            logger.info("ðŸ”ª Step 1: Running ingestion workflow...")
            result = graph.invoke(state, config={"configurable": {"thread_id": "test_thread_1"}})
            
            # LangGraph returns a dict when using checkpointer
            assert result["chunks"] is not None
            assert len(result["chunks"]) > 1  # Should be split into multiple chunks
            logger.info(f"âœ… Chunked into {len(result['chunks'])} chunks")
            
            # Verify chunk sizes are reasonable
            chunk_sizes = [len(chunk) for chunk in result["chunks"]]
            logger.info(f"ðŸ“Š Chunk sizes: min={min(chunk_sizes)}, max={max(chunk_sizes)}, avg={sum(chunk_sizes)/len(chunk_sizes):.1f}")
            
            # Verify embeddings were generated
            assert result["embeddings"] is not None
            assert len(result["embeddings"]) == len(result["chunks"])
            logger.info(f"âœ… Generated {len(result['embeddings'])} embeddings")
            
            # Verify embedding dimensions
            embedding_dims = [len(emb) for emb in result["embeddings"]]
            assert all(dim == embedding_dims[0] for dim in embedding_dims)  # All should have same dimensions
            logger.info(f"ðŸ”¤ Embedding dimensions: {embedding_dims[0]}")
            
            # Verify storage was successful
            assert result["status"] == "stored"
            logger.info(f"âœ… Stored successfully in ChromaDB")
            
            # Verify vectorstore was created and populated
            assert real_graph_builder.vectorstore is not None
            
            # Test retrieval from the stored data
            logger.info("ðŸ” Step 4: Testing retrieval...")
            retriever = real_graph_builder.vectorstore.as_retriever(search_kwargs={"k": 3})
            real_graph_builder.retriever = retriever
            
            query_state = KnowledgeState(
                query_type="query",
                user_input="What is the Smart Second Brain system?",
                messages=[{"role": "user", "content": "What is the Smart Second Brain system?"}]
            )
            
            # Run the complete query workflow with checkpointer
            logger.info("ðŸ¤– Step 5: Testing answer generation...")
            query_result = graph.invoke(query_state, config={"configurable": {"thread_id": "test_thread_2"}})
            
            # LangGraph returns a dict when using checkpointer
            assert query_result["retrieved_docs"] is not None
            assert len(query_result["retrieved_docs"]) > 0
            logger.info(f"âœ… Retrieved {len(query_result['retrieved_docs'])} relevant documents")
            
            assert query_result["generated_answer"] is not None
            assert len(query_result["generated_answer"]) > 50
            logger.info(f"âœ… Generated answer: {query_result['generated_answer'][:100]}...")
            
            assert query_result["status"] == "validated"
            logger.info("âœ… Complete workflow successful!")
            
            # Summary
            logger.info(f"ðŸ“Š Workflow Summary:")
            logger.info(f"   - Input: {len(long_paragraph)} characters")
            logger.info(f"   - Chunks: {len(result['chunks'])}")
            logger.info(f"   - Embeddings: {len(result['embeddings'])}")
            logger.info(f"   - Embedding dimensions: {embedding_dims[0]}")
            logger.info(f"   - Retrieved docs: {len(query_result['retrieved_docs'])}")
            logger.info(f"   - Final status: {query_result['status']}")
            
        except Exception as e:
            pytest.fail(f"Long paragraph workflow test failed: {e}")

    @pytest.mark.integration
    def test_query_workflow(self, real_graph_builder):
        """Test the complete query workflow: retriever -> answer -> review -> validated_store."""
        if not os.getenv("OPENAI_API_KEY"):
            pytest.skip("OPENAI_API_KEY not set in .env file")

        try:
            logger.info("ðŸ” Testing Query Workflow")
            logger.info("=" * 40)

            # Ensure vectorstore is available for the workflow
            if not real_graph_builder.vectorstore:
                logger.info("ðŸ”§ Setting up vectorstore for test...")
                # Create vectorstore directly
                from langchain_chroma import Chroma
                real_graph_builder.vectorstore = Chroma(
                    collection_name="test_knowledge_base",
                    embedding_function=real_graph_builder.embedding_model,
                    persist_directory=real_graph_builder.chromadb_dir
                )
                logger.info("âœ… Vectorstore initialized")

            # Build the graph with checkpointer
            graph = real_graph_builder.build()

            # Create a query state
            query_state = KnowledgeState(
                query_type="query",
                user_input="What are the key features of artificial intelligence systems?",
                messages=[{"role": "user", "content": "What are the key features of artificial intelligence systems?"}],
                categories=["ai", "technology"],
                source="test_query"
            )

            logger.info(f"Query: '{query_state.user_input}'")

            # Step 1: Add sample documents to ensure we have content to retrieve
            logger.info("ðŸ” Step 1: Setting up test documents...")
            sample_doc = """
            Artificial Intelligence (AI) systems have several key features:
            1. Learning capability - ability to improve performance over time
            2. Reasoning - logical inference and problem-solving
            3. Perception - sensing and interpreting the environment
            4. Natural language understanding - processing human language
            5. Adaptability - adjusting to new situations and data

            These features enable AI systems to perform complex tasks like image recognition,
            language translation, autonomous driving, and medical diagnosis.
            """

            from langchain.schema import Document
            real_graph_builder.vectorstore.add_documents([
                Document(
                    page_content=sample_doc,
                    metadata={"source": "ai_features", "categories": "ai, technology"}
                )
            ])
            logger.info("âœ… Added sample document to vectorstore")

            # Run the complete query workflow
            logger.info("ðŸ¤– Step 2: Running complete query workflow...")
            result = graph.invoke(query_state, config={"configurable": {"thread_id": "query_test_thread"}})

            # Debug: Print all keys in result
            logger.info(f"ðŸ” Result keys: {list(result.keys())}")
            for key, value in result.items():
                logger.info(f"  {key}: {type(value)} = {str(value)[:100]}{'...' if len(str(value)) > 100 else ''}")

            # Verify retrieval results
            assert result["retrieved_docs"] is not None
            retrieved_count = len(result["retrieved_docs"])
            logger.info(f"âœ… Retrieved {retrieved_count} documents")

            # Verify answer generation
            assert result["generated_answer"] is not None
            answer_length = len(result["generated_answer"])
            assert answer_length > 10  # At least some content
            logger.info(f"âœ… Generated answer ({answer_length} characters)")

            # Verify the answer contains relevant information
            answer_text = result["generated_answer"].lower()
            assert any(keyword in answer_text for keyword in ["ai", "artificial intelligence", "learning", "reasoning"])

            # Check if status exists, if not, the workflow completed successfully
            if "status" in result:
                logger.info(f"âœ… Final status: {result['status']}")
                assert result["status"] == "validated"
            else:
                logger.info("â„¹ï¸  Status not set, but workflow completed")

            # Check for human feedback
            if "human_feedback" in result:
                logger.info(f"âœ… Human feedback: {result['human_feedback']}")
                assert result["human_feedback"] == "approved"
            else:
                logger.info("â„¹ï¸  No human feedback set")

            # Verify logs contain expected entries
            logs = result.get("logs", [])
            assert len(logs) > 0
            logger.info(f"âœ… Workflow logs: {len(logs)} entries")
            log_text = " ".join(logs).lower()
            assert any(keyword in log_text for keyword in ["retrieved", "generated", "approved"])

            logger.info("ðŸŽ‰ Query workflow test completed successfully!")

        except Exception as e:
            logger.error(f"âŒ Query workflow test failed: {e}")
            import traceback
            traceback.print_exc()
            pytest.fail(f"Query workflow test failed: {e}")

    @pytest.mark.integration
    def test_query_workflow_with_empty_vectorstore(self, real_graph_builder):
        """Test query workflow when vectorstore is empty."""
        if not os.getenv("OPENAI_API_KEY"):
            pytest.skip("OPENAI_API_KEY not set in .env file")

        try:
            logger.info("ðŸ” Testing Query Workflow with Empty VectorStore")

            # Build the graph with checkpointer
            graph = real_graph_builder.build()

            # Create a query state
            query_state = KnowledgeState(
                query_type="query",
                user_input="What is machine learning?",
                messages=[{"role": "user", "content": "What is machine learning?"}]
            )

            # Run the query workflow
            result = graph.invoke(query_state, config={"configurable": {"thread_id": "empty_vs_test"}})

                        # Should still generate an answer (even if based on limited/no context)
            assert result["generated_answer"] is not None

            # Status might not be set if vectorstore operations are skipped
            if "status" in result:
                logger.info(f"âœ… Status: {result['status']}")
            else:
                logger.info("â„¹ï¸  No status set (expected for empty vectorstore)")

            # Should have retrieved docs array (possibly empty)
            assert isinstance(result["retrieved_docs"], list)

            logger.info("âœ… Empty vectorstore query handled gracefully")

        except Exception as e:
            logger.error(f"âŒ Empty vectorstore test failed: {e}")
            pytest.fail(f"Empty vectorstore test failed: {e}")

    @pytest.mark.integration
    def test_query_workflow_error_handling(self, real_graph_builder):
        """Test query workflow error handling."""
        if not os.getenv("OPENAI_API_KEY"):
            pytest.skip("OPENAI_API_KEY not set in .env file")

        try:
            logger.info("ðŸ” Testing Query Workflow Error Handling")

            # Build the graph with checkpointer
            graph = real_graph_builder.build()

            # Test with invalid query state
            invalid_state = KnowledgeState(
                query_type="query",
                user_input="",  # Empty query
                messages=[]
            )

            # Should handle gracefully
            result = graph.invoke(invalid_state, config={"configurable": {"thread_id": "error_test"}})

            # Should still complete the workflow
            if "status" in result:
                assert result["status"] in ["validated", "error"]
                logger.info(f"âœ… Status: {result['status']}")
            else:
                logger.info("â„¹ï¸  No status set (expected for error cases)")

            assert "logs" in result

            logger.info("âœ… Error handling test completed")

        except Exception as e:
            logger.error(f"âŒ Error handling test failed: {e}")
            pytest.fail(f"Error handling test failed: {e}")

    def test_store_node(self, graph_builder, sample_ingest_state, mock_vectorstore):
        """Test store node with vectorstore."""
        # Prepare state with chunks and embeddings
        state = graph_builder.chunk_doc_node(sample_ingest_state)
        state = graph_builder.embed_node(state)
        result_state = graph_builder.store_node(state)
        
        assert result_state.status == "stored"
        # Verify vectorstore was called
        mock_vectorstore.add_texts.assert_called()

    def test_store_node_no_vectorstore(self, sample_ingest_state):
        """Test store node without vectorstore."""
        builder = MasterGraphBuilder()  # No vectorstore
        state = builder.chunk_doc_node(sample_ingest_state)
        state = builder.embed_node(state)
        result_state = builder.store_node(state)

        assert result_state.status == "error"  # Expected when no vectorstore

    def test_retriever_node(self, graph_builder, sample_query_state, mock_retriever):
        """Test retriever node with retriever."""
        result_state = graph_builder.retriever_node(sample_query_state)
        
        assert result_state.retrieved_docs is not None
        assert len(result_state.retrieved_docs) == 1
        assert "This is a retrieved document for testing." in result_state.retrieved_docs[0]["content"]
        
        # Verify retriever was called
        mock_retriever.get_relevant_documents.assert_called_with(sample_query_state.user_input)

    def test_retriever_node_no_retriever(self, sample_query_state):
        """Test retriever node without retriever."""
        builder = MasterGraphBuilder()  # No retriever
        result_state = builder.retriever_node(sample_query_state)

        assert result_state.retrieved_docs is not None
        assert len(result_state.retrieved_docs) == 0  # No retriever available

    def test_answer_gen_node(self, graph_builder, sample_query_state, mock_llm):
        """Test answer generation node with LLM."""
        # Prepare state with retrieved docs
        state = graph_builder.retriever_node(sample_query_state)
        
        # Mock the file read for answer_prompt.txt
        mock_prompt_content = """You are a helpful knowledge assistant with access to conversation history.

CONVERSATION HISTORY:
{conversation}

CURRENT USER QUESTION:
{query}

RETRIEVED KNOWLEDGE BASE CONTEXT:
{context}

INSTRUCTIONS:
- Use the conversation history to understand context and references
- Base your answer primarily on the retrieved knowledge base context
- If the knowledge base context is insufficient, say "I don't know based on available knowledge."
- Consider previous questions and answers to provide better context
- Keep the answer clear, concise, and contextually relevant
- If the user refers to something from earlier in the conversation, acknowledge it

"""
        
        with patch("builtins.open", mock_open(read_data=mock_prompt_content)), \
             patch("agentic.workflows.master_graph_builder.ChatPromptTemplate") as mock_prompt_class:
            
            # Mock the prompt template and chain
            mock_prompt = Mock()
            mock_chain = Mock()
            mock_chain.invoke.return_value = mock_llm.invoke.return_value
            
            # Add __or__ method to mock_prompt
            def mock_or(self, other):
                return mock_chain
            mock_prompt.__or__ = mock_or
            
            mock_prompt_class.from_template.return_value = mock_prompt
            
            result_state = graph_builder.answer_gen_node(state)
        
        assert result_state.generated_answer is not None
        assert isinstance(result_state.generated_answer, str)
        assert "This is a test answer generated by the LLM." in result_state.generated_answer
        assert len(result_state.messages) == 3  # Original user + user input + AI response
        
        # Verify chain was called
        mock_chain.invoke.assert_called()

    def test_answer_gen_node_no_llm(self, sample_query_state):
        """Test answer generation node without LLM."""
        builder = MasterGraphBuilder()  # No LLM
        state = builder.retriever_node(sample_query_state)
        result_state = builder.answer_gen_node(state)
        
        assert result_state.generated_answer is None

    def test_human_review_node(self, graph_builder, sample_query_state):
        """Test human review node."""
        # Prepare state with generated answer
        state = graph_builder.retriever_node(sample_query_state)
        state = graph_builder.answer_gen_node(state)
        result_state = graph_builder.human_review_node(state)
        
        assert result_state.human_feedback == "approved"
        assert result_state.final_answer == result_state.generated_answer

    def test_validated_store_node(self, graph_builder, sample_query_state, mock_vectorstore):
        """Test validated store node with vectorstore."""
        # Prepare state with final answer
        state = graph_builder.retriever_node(sample_query_state)
        state = graph_builder.answer_gen_node(state)
        state = graph_builder.human_review_node(state)
        result_state = graph_builder.validated_store_node(state)
        
        assert result_state.status == "validated"  # Default status without vector storage
        # Vectorstore should not be called since knowledge_type is not "reusable" or "verified"
        mock_vectorstore.add_documents.assert_not_called()

    def test_validated_store_node_no_vectorstore(self, sample_query_state):
        """Test validated store node without vectorstore."""
        builder = MasterGraphBuilder()  # No vectorstore
        state = builder.retriever_node(sample_query_state)
        state = builder.answer_gen_node(state)
        state = builder.human_review_node(state)
        result_state = builder.validated_store_node(state)
        
        assert result_state.status is None  # No final_answer to validate

    def test_build_graph(self, graph_builder):
        """Test graph compilation."""
        compiled_graph = graph_builder.build()
        
        assert compiled_graph is not None
        # Verify the graph has the expected structure
        # Note: We can't easily test the internal structure without accessing private attributes

    def test_full_ingest_workflow(self, graph_builder, sample_ingest_state, mock_vectorstore):
        """Test complete ingest workflow."""
        compiled_graph = graph_builder.build()
        
        # Run the workflow with thread_id for checkpointer
        result = compiled_graph.invoke(sample_ingest_state, config={"configurable": {"thread_id": "test_thread"}})
        
        # LangGraph returns a dict with the final state
        final_state = result if isinstance(result, dict) else result
        assert final_state["status"] == "stored"
        assert final_state["chunks"] is not None
        assert final_state["embeddings"] is not None
        # Verify vectorstore was called
        mock_vectorstore.add_texts.assert_called()

    def test_full_query_workflow(self, graph_builder, sample_query_state, mock_llm, mock_retriever, mock_vectorstore):
        """Test complete query workflow."""
        compiled_graph = graph_builder.build()
        
        # Mock the prompt template for the full workflow
        with patch("agentic.workflows.master_graph_builder.ChatPromptTemplate") as mock_prompt_class:
            mock_prompt = Mock()
            mock_chain = Mock()
            mock_chain.invoke.return_value = mock_llm.invoke.return_value
            
            def mock_or(self, other):
                return mock_chain
            mock_prompt.__or__ = mock_or
            
            mock_prompt_class.from_template.return_value = mock_prompt
            
            # Run the workflow with thread_id for checkpointer
            result = compiled_graph.invoke(sample_query_state, config={"configurable": {"thread_id": "test_thread"}})
        
        # LangGraph returns a dict with the final state
        final_state = result if isinstance(result, dict) else result
        assert final_state["status"] == "validated"
        assert final_state["generated_answer"] is not None
        assert final_state["final_answer"] is not None
        assert final_state["human_feedback"] == "approved"
        assert final_state["retrieved_docs"] is not None
        
        # Verify all components were called
        mock_retriever.get_relevant_documents.assert_called()
        # Note: mock_llm.invoke is not called directly since we mock the chain
        # Note: add_texts is not called when knowledge_type is None (vector storage skipped)

    def test_error_handling(self, graph_builder):
        """Test error handling in the workflow."""
        compiled_graph = graph_builder.build()
        
        # Test with invalid state
        invalid_state = KnowledgeState(query_type="invalid")
        result = compiled_graph.invoke(invalid_state, config={"configurable": {"thread_id": "test_thread"}})
        
        # LangGraph returns a dict with the final state
        final_state = result if isinstance(result, dict) else result
        assert final_state["status"] == "error"
        assert "Unknown query_type" in (final_state["logs"] or [])

    # ============================================================================
    # REAL INTEGRATION TESTS
    # ============================================================================

    @pytest.mark.integration
    def test_real_llm_connection(self, real_graph_builder):
        """Test direct LLM connection and response."""
        try:
            # Test a simple prompt
            test_prompt = [
                {"role": "system", "content": "You are a helpful assistant."},
                {"role": "user", "content": "Say 'Hello from Smart Second Brain!' and nothing else."}
            ]
            
            response = real_graph_builder.llm.invoke(test_prompt)
            
            assert response is not None
            assert response.content is not None
            assert len(response.content) > 0
            
            logger.info("ðŸ”— LLM Connection Test:")
            logger.info(f"   Response: {response.content}")
            logger.info(f"   Model: {real_graph_builder.llm.model_name}")
            logger.info(f"   Provider: {'Azure OpenAI' if 'azure' in str(real_graph_builder.llm.openai_api_base).lower() else 'OpenAI'}")
            
        except Exception as e:
            logger.error(f"LLM connection test failed: {e}")
            pytest.fail(f"LLM connection test failed: {e}")

    @pytest.mark.integration
    def test_real_llm_answer_generation(self, real_graph_builder, sample_query_state):
        """Test answer generation with real OpenAI LLM."""
        # Prepare state with retrieved docs
        state = real_graph_builder.retriever_node(sample_query_state)
        result_state = real_graph_builder.answer_gen_node(state)
        
        assert result_state.generated_answer is not None
        assert len(result_state.generated_answer) > 10
        assert len(result_state.messages) == 3  # Original user + user input + AI response
        
        logger.info(f"ðŸ¤– Real LLM Response: {result_state.generated_answer}")

    @pytest.mark.integration
    def test_real_document_ingestion_with_vectorstore(self, real_graph_builder, sample_documents):
        """Test document ingestion with real vector store."""
        if not os.getenv("OPENAI_API_KEY"):
            pytest.skip("OPENAI_API_KEY not set in .env file")
        
        try:
            # Create real vector store with proper API configuration
            openai_api_key = os.getenv("OPENAI_API_KEY")
            azure_endpoint = os.getenv("AZURE_OPENAI_ENDPOINT_URL")
            
            # Get embedding model from environment
            embedding_model = os.getenv("EMBEDDING_MODEL", "text-embedding-3-small")
            api_version = os.getenv("API_VERSION", "2024-02-15-preview")
            
            if azure_endpoint and azure_endpoint != "https://your-resource-name.openai.azure.com/":
                # Use AzureOpenAIEmbeddings for Azure endpoints
                deployment = "text-embedding-3-small"
                logger.info(f"ðŸ”— Using Azure OpenAI embeddings: {embedding_model}")
                logger.info(f"ðŸš€ Using deployment: {deployment}")
                embeddings = AzureOpenAIEmbeddings(
                    azure_deployment=deployment,
                    openai_api_version=api_version,
                    azure_endpoint=azure_endpoint,
                    openai_api_key=openai_api_key
                )
            else:
                embeddings = OpenAIEmbeddings(
                    model=embedding_model,
                    openai_api_key=openai_api_key
                )
            
            # Use the temporary ChromaDB directory from the fixture
            vectorstore = Chroma(
                collection_name="knowledge_base",
                embedding_function=embeddings,
                persist_directory=real_graph_builder.chromadb_dir
            )
            
            # Update builder with real vector store
            real_graph_builder.vectorstore = vectorstore
            
            # Test document ingestion
            for i, doc_text in enumerate(sample_documents):
                state = KnowledgeState(
                    query_type="ingest",
                    raw_document=doc_text,
                    metadata={"source": f"test_doc_{i}", "timestamp": "2024-01-01T00:00:00Z"}
                )
                
                # Run ingestion workflow
                compiled_graph = real_graph_builder.build()
                result = compiled_graph.invoke(state, config={"configurable": {"thread_id": f"test_thread_{i}"}})
                
                # LangGraph returns a dict, not a KnowledgeState object
                assert result["status"] == "stored"
                assert result["chunks"] is not None
                assert result["embeddings"] is not None
                assert len(result["chunks"]) > 0
                assert len(result["embeddings"]) == len(result["chunks"])
                
                logger.info(f"âœ… Document {i+1} ingested successfully")
            
            # Test retrieval
            query_state = KnowledgeState(
                query_type="query",
                user_input="What is a knowledge graph?",
                messages=[{"role": "user", "content": "What is a knowledge graph?"}]
            )
            
            # Create retriever from vector store
            retriever = vectorstore.as_retriever(search_kwargs={"k": 2})
            real_graph_builder.retriever = retriever
            
            # Run query workflow
            result = compiled_graph.invoke(query_state, config={"configurable": {"thread_id": "test_query_thread"}})
            
            # LangGraph returns a dict, not a KnowledgeState object
            assert result["status"] == "validated"
            assert result["generated_answer"] is not None
            assert result["retrieved_docs"] is not None
            assert len(result["retrieved_docs"]) > 0
            
            logger.info(f"ðŸ” Retrieved {len(result['retrieved_docs'])} documents")
            logger.info(f"ðŸ¤– Generated Answer: {result['generated_answer']}")
            
        except Exception as e:
            pytest.fail(f"Real integration test failed: {e}")

    @pytest.mark.integration
    def test_real_workflow_with_text_splitter(self, real_graph_builder, sample_documents):
        """Test complete workflow with real text splitting and embeddings."""
        if not os.getenv("OPENAI_API_KEY"):
            pytest.skip("OPENAI_API_KEY not set in .env file")
        
        try:
            # Create real components with proper API configuration
            openai_api_key = os.getenv("OPENAI_API_KEY")
            azure_endpoint = os.getenv("AZURE_OPENAI_ENDPOINT_URL")
            
            # Get embedding model and API version from environment
            embedding_model = os.getenv("EMBEDDING_MODEL", "text-embedding-3-small")
            api_version = os.getenv("API_VERSION", "2024-02-15-preview")
            
            if azure_endpoint and azure_endpoint != "https://your-resource-name.openai.azure.com/":
                # Use AzureOpenAIEmbeddings for Azure endpoints
                logger.info(f"ðŸ”— Using Azure OpenAI embeddings: {embedding_model}")
                embeddings = AzureOpenAIEmbeddings(
                    azure_deployment=embedding_model,
                    openai_api_version=api_version,
                    azure_endpoint=azure_endpoint,
                    openai_api_key=openai_api_key
                )
            else:
                embeddings = OpenAIEmbeddings(
                    model=embedding_model,
                    openai_api_key=openai_api_key
                )
            
            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=100,
                chunk_overlap=20,
                separators=["\n\n", "\n", " ", ""]
            )
            # Use the temporary ChromaDB directory from the fixture
            vectorstore = Chroma(
                collection_name="knowledge_base",
                embedding_function=embeddings,
                persist_directory=real_graph_builder.chromadb_dir
            )
            
            # Update builder
            real_graph_builder.vectorstore = vectorstore
            
            # Override chunk_doc_node to use real text splitter
            original_chunk_method = real_graph_builder.chunk_doc_node
            
            def real_chunk_doc_node(state: KnowledgeState):
                if state.raw_document:
                    chunks = text_splitter.split_text(state.raw_document)
                    state.chunks = chunks
                return state
            
            real_graph_builder.chunk_doc_node = real_chunk_doc_node
            
            # Test with a longer document
            long_document = "\n\n".join(sample_documents)
            state = KnowledgeState(
                query_type="ingest",
                raw_document=long_document,
                metadata={"source": "integration_test", "timestamp": "2024-01-01T00:00:00Z"}
            )
            
            # Run ingestion
            compiled_graph = real_graph_builder.build()
            result = compiled_graph.invoke(state, config={"configurable": {"thread_id": "test_workflow_thread"}})
            
            # LangGraph returns a dict, not a KnowledgeState object
            assert result["status"] == "stored"
            assert result["chunks"] is not None
            assert len(result["chunks"]) > 1  # Should be split into multiple chunks
            
            logger.info(f"âœ… Document split into {len(result['chunks'])} chunks")
            
            # Test retrieval and answer generation
            query_state = KnowledgeState(
                query_type="query",
                user_input="Explain the Smart Second Brain system",
                messages=[{"role": "user", "content": "Explain the Smart Second Brain system"}]
            )
            
            retriever = vectorstore.as_retriever(search_kwargs={"k": 3})
            real_graph_builder.retriever = retriever
            
            result = compiled_graph.invoke(query_state, config={"configurable": {"thread_id": "test_workflow_query_thread"}})
            
            # LangGraph returns a dict, not a KnowledgeState object
            assert result["status"] == "validated"
            assert result["generated_answer"] is not None
            assert len(result["generated_answer"]) > 50
            
            logger.info(f"ðŸ¤– Final Answer: {result['generated_answer']}")
            
        except Exception as e:
            pytest.fail(f"Real workflow test failed: {e}")

    @pytest.mark.integration
    def test_real_performance_benchmark(self, real_graph_builder, sample_documents):
        """Benchmark performance with real components."""
        if not os.getenv("OPENAI_API_KEY"):
            pytest.skip("OPENAI_API_KEY not set in .env file")
        
        import time
        
        try:
            # Setup real components with proper API configuration
            openai_api_key = os.getenv("OPENAI_API_KEY")
            azure_endpoint = os.getenv("AZURE_OPENAI_ENDPOINT_URL")
            
            # Get embedding model and API version from environment
            embedding_model = os.getenv("EMBEDDING_MODEL", "text-embedding-3-small")
            api_version = os.getenv("API_VERSION", "2024-02-15-preview")
            
            if azure_endpoint and azure_endpoint != "https://your-resource-name.openai.azure.com/":
                # Use AzureOpenAIEmbeddings for Azure endpoints
                deployment = "text-embedding-3-small"
                logger.info(f"ðŸ”— Using Azure OpenAI embeddings: {embedding_model}")
                logger.info(f"ðŸš€ Using deployment: {deployment}")
                embeddings = AzureOpenAIEmbeddings(
                    azure_deployment=deployment,
                    openai_api_version=api_version,
                    azure_endpoint=azure_endpoint,
                    openai_api_key=openai_api_key
                )
            else:
                embeddings = OpenAIEmbeddings(
                    model=embedding_model,
                    openai_api_key=openai_api_key
                )
            
            # Use the temporary ChromaDB directory from the fixture
            vectorstore = Chroma(
                collection_name="knowledge_base",
                embedding_function=embeddings,
                persist_directory=real_graph_builder.chromadb_dir
            )
            real_graph_builder.vectorstore = vectorstore
            
            # Benchmark ingestion
            start_time = time.time()
            
            for i, doc_text in enumerate(sample_documents[:3]):  # Test with first 3 docs
                state = KnowledgeState(
                    query_type="ingest",
                    raw_document=doc_text,
                    metadata={"source": f"benchmark_doc_{i}"}
                )
                
                compiled_graph = real_graph_builder.build()
                result = compiled_graph.invoke(state, config={"configurable": {"thread_id": f"benchmark_thread_{i}"}})
                
                # LangGraph returns a dict, not a KnowledgeState object
                assert result["status"] == "stored"
            
            ingestion_time = time.time() - start_time
            logger.info(f"â±ï¸  Ingestion time for 3 documents: {ingestion_time:.2f} seconds")
            
            # Benchmark query
            start_time = time.time()
            
            retriever = vectorstore.as_retriever(search_kwargs={"k": 2})
            real_graph_builder.retriever = retriever
            
            query_state = KnowledgeState(
                query_type="query",
                user_input="What are the key features of AI systems?",
                messages=[{"role": "user", "content": "What are the key features of AI systems?"}]
            )
            
            result = compiled_graph.invoke(query_state, config={"configurable": {"thread_id": "benchmark_query_thread"}})
            
            query_time = time.time() - start_time
            logger.info(f"â±ï¸  Query time: {query_time:.2f} seconds")
            
            # LangGraph returns a dict, not a KnowledgeState object
            assert result["status"] == "validated"
            assert result["generated_answer"] is not None
            
            logger.info(f"ðŸ“Š Performance Summary:")
            logger.info(f"   - Ingestion: {ingestion_time:.2f}s for 3 docs")
            logger.info(f"   - Query: {query_time:.2f}s")
            logger.info(f"   - Total: {ingestion_time + query_time:.2f}s")
            
        except Exception as e:
            pytest.fail(f"Performance benchmark failed: {e}")


if __name__ == "__main__":
    # Run the tests
    pytest.main([__file__, "-v"])
